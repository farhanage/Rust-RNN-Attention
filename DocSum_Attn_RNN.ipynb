{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Document Summarization with Attention RNN (PyTorch)\n",
        "This notebook mirrors the Rust project:\n",
        "- Bidirectional **Encoder LSTM** and unidirectional **Decoder LSTM**\n",
        "- **Bahdanau Attention** with decoder-hidden projection to encoder-dim\n",
        "- **Masked cross-entropy** ignoring PAD and OOV\n",
        "- **Greedy decoding** for generation\n",
        "- **ROUGE-1 / ROUGE-2 / ROUGE-L** metrics (no external deps)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Optional:** install packages (skip if already installed).\n",
        "```bash\n",
        "pip install torch torchvision torchaudio pandas numpy tqdm\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Data expectation**\n",
        "- CSV file with columns: `text` and `summary`.\n",
        "- Default path is `../Data.csv` (same as your Rust run). Change `DATA_CSV` below if needed.\n",
        "\n",
        "**Notes**\n",
        "- PAD=0, SOS=1, EOS=2, UNK=3 (mirrors the Rust tokenizer setup we used).\n",
        "- Notebook picks **CUDA** if available; else CPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Torch: 2.4.1\n",
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import math, random, os, re, time\n",
        "from typing import List, Tuple, Dict\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "\n",
        "print(\"Torch:\", torch.__version__)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Hyperparams mirroring Rust constants\n",
        "EMBEDDING_DIM = 256\n",
        "HIDDEN_DIM    = 512\n",
        "NUM_LAYERS    = 2\n",
        "BATCH_SIZE    = 8\n",
        "MAX_SEQ_LEN   = 512\n",
        "MAX_SUMMARY_LEN = 128\n",
        "EPOCHS        = 5\n",
        "LEARNING_RATE = 1e-4\n",
        "\n",
        "# Data path (same default as Rust)\n",
        "DATA_CSV = \"Data.csv\"\n",
        "RNG_SEED = 42\n",
        "random.seed(RNG_SEED)\n",
        "np.random.seed(RNG_SEED)\n",
        "torch.manual_seed(RNG_SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(RNG_SEED)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 2225 samples\n",
            "Unique texts: 2127\n",
            "Unique summaries: 2081\n",
            "Train: 2002 | Val: 223\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def load_data(path: str):\n",
        "    if os.path.exists(path):\n",
        "        df = pd.read_csv(path)\n",
        "        text_col, summary_col = None, None\n",
        "        for c in df.columns:\n",
        "            lc = c.strip().lower()\n",
        "            if lc == \"text\": text_col = c\n",
        "            if lc == \"summary\": summary_col = c\n",
        "        if text_col is None or summary_col is None:\n",
        "            raise ValueError(\"CSV must have 'text' and 'summary' columns.\")\n",
        "        data = list(zip(df[text_col].astype(str).tolist(), df[summary_col].astype(str).tolist()))\n",
        "        return data\n",
        "    else:\n",
        "        print(\"WARNING: File not found:\", path)\n",
        "        print(\"Using tiny fallback samples for a dry run.\\n\")\n",
        "        samples = [\n",
        "            (\"Pemerintah mengumumkan kebijakan baru untuk mendukung UMKM di tengah kondisi ekonomi yang menantang.\",\n",
        "             \"Pemerintah keluarkan kebijakan dukung UMKM.\"),\n",
        "            (\"Tim sepak bola nasional berhasil menang 2-1 dalam pertandingan persahabatan melawan negara tetangga.\",\n",
        "             \"Timnas menang 2-1 di laga persahabatan.\"),\n",
        "            (\"Peneliti menemukan metode baru untuk meningkatkan efisiensi panel surya melalui material perovskite.\",\n",
        "             \"Metode baru tingkatkan efisiensi panel surya.\"),\n",
        "            (\"Sebuah perusahaan teknologi meluncurkan ponsel pintar dengan fitur kamera inovatif dan baterai tahan lama.\",\n",
        "             \"Ponsel baru diluncurkan dengan kamera inovatif dan baterai awet.\"),\n",
        "            (\"Konser musik akbar akan digelar akhir pekan ini dengan menghadirkan musisi lokal dan internasional.\",\n",
        "             \"Konser akbar akhir pekan hadirkan musisi lokal dan internasional.\"),\n",
        "        ]\n",
        "        return samples\n",
        "\n",
        "data = load_data(DATA_CSV)\n",
        "print(f\"Loaded {len(data)} samples\")\n",
        "texts = [t for t,_ in data]; sums  = [s for _,s in data]\n",
        "print(\"Unique texts:\", len(set(texts)))\n",
        "print(\"Unique summaries:\", len(set(sums)))\n",
        "split = int(0.9 * len(data))\n",
        "train_data = data[:split]\n",
        "val_data   = data[split:] if split < len(data) else data\n",
        "print(\"Train:\", len(train_data), \"| Val:\", len(val_data))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocab size: 10000\n"
          ]
        }
      ],
      "source": [
        "\n",
        "PAD, SOS, EOS, UNK = 0, 1, 2, 3\n",
        "\n",
        "class Tokenizer:\n",
        "    def __init__(self, vocab_size: int = 10000):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.word2id: Dict[str, int] = {}\n",
        "        self.id2word: List[str] = []\n",
        "\n",
        "    def _basic_tokens(self, s: str) -> List[str]:\n",
        "        toks = []\n",
        "        for t in s.lower().split():\n",
        "            t = \"\".join(ch for ch in t if ch.isalnum())\n",
        "            if t:\n",
        "                toks.append(t)\n",
        "        return toks\n",
        "\n",
        "    def fit(self, pairs: List[Tuple[str, str]]):\n",
        "        from collections import Counter\n",
        "        cnt = Counter()\n",
        "        for text, summ in pairs:\n",
        "            cnt.update(self._basic_tokens(text))\n",
        "            cnt.update(self._basic_tokens(summ))\n",
        "        vocab = [\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"]\n",
        "        for w,_ in cnt.most_common(self.vocab_size - len(vocab)):\n",
        "            if w not in vocab:\n",
        "                vocab.append(w)\n",
        "        self.id2word = vocab\n",
        "        self.word2id = {w:i for i,w in enumerate(self.id2word)}\n",
        "\n",
        "    def encode(self, s: str) -> List[int]:\n",
        "        return [ self.word2id.get(tok, UNK) for tok in self._basic_tokens(s) ]\n",
        "\n",
        "    def encode_with_special(self, s: str, max_len: int, add_sos: bool=False, add_eos: bool=True) -> List[int]:\n",
        "        toks = self.encode(s)\n",
        "        if add_sos: toks = [SOS] + toks\n",
        "        if add_eos: toks = toks + [EOS]\n",
        "        toks = toks[:max_len]\n",
        "        if len(toks) < max_len:\n",
        "            toks = toks + [PAD]*(max_len - len(toks))\n",
        "        return toks\n",
        "\n",
        "    def decode(self, ids: List[int]) -> str:\n",
        "        words = []\n",
        "        for i in ids:\n",
        "            if i == EOS: break\n",
        "            if i in (PAD, SOS): \n",
        "                continue\n",
        "            if 0 <= i < len(self.id2word):\n",
        "                w = self.id2word[i]\n",
        "                if w not in (\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"):\n",
        "                    words.append(w)\n",
        "        return \" \".join(words)\n",
        "\n",
        "    def vocab_size_(self) -> int:\n",
        "        return len(self.id2word)\n",
        "\n",
        "tokenizer = Tokenizer(vocab_size=10000)\n",
        "tokenizer.fit(train_data)\n",
        "V = tokenizer.vocab_size_()\n",
        "print(\"Vocab size:\", V)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def make_batch(pairs: List[Tuple[str,str]], max_text_len: int, max_sum_len: int, device):\n",
        "    src_batch, tgt_in_batch, tgt_out_batch, src_mask = [], [], [], []\n",
        "    for text, summ in pairs:\n",
        "        src = tokenizer.encode_with_special(text, max_text_len, add_sos=False, add_eos=True)\n",
        "        tgt_in  = tokenizer.encode_with_special(summ, max_sum_len, add_sos=True,  add_eos=False)\n",
        "        tgt_out = tokenizer.encode_with_special(summ, max_sum_len, add_sos=False, add_eos=True)\n",
        "        mask = [0 if x==PAD else 1 for x in src]\n",
        "        src_batch.append(src)\n",
        "        tgt_in_batch.append(tgt_in)\n",
        "        tgt_out_batch.append(tgt_out)\n",
        "        src_mask.append(mask)\n",
        "\n",
        "    src = torch.tensor(src_batch, dtype=torch.long, device=device)          # [B, S]\n",
        "    tgt_in = torch.tensor(tgt_in_batch, dtype=torch.long, device=device)    # [B, T]\n",
        "    tgt_out = torch.tensor(tgt_out_batch, dtype=torch.long, device=device)  # [B, T]\n",
        "    mask = torch.tensor(src_mask, dtype=torch.bool, device=device)          # [B, S], True=valid\n",
        "    return src, tgt_in, tgt_out, mask\n",
        "\n",
        "def batch_iter(data, batch_size):\n",
        "    for i in range(0, len(data), batch_size):\n",
        "        yield data[i:i+batch_size]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model ready\n"
          ]
        }
      ],
      "source": [
        "\n",
        "class BahdanauAttention(nn.Module):\n",
        "    def __init__(self, hidden_dim_enc: int):\n",
        "        super().__init__()\n",
        "        self.w_query = nn.Linear(hidden_dim_enc, hidden_dim_enc, bias=True)\n",
        "        self.w_key   = nn.Linear(hidden_dim_enc, hidden_dim_enc, bias=True)\n",
        "        self.v       = nn.Linear(hidden_dim_enc, 1, bias=True)\n",
        "\n",
        "    def forward(self, query: Tensor, keys: Tensor, mask: Tensor=None):\n",
        "        # query: [B, hidden_enc], keys: [B, S, hidden_enc], mask: [B,S] Bool\n",
        "        B, S, H = keys.shape\n",
        "        q = self.w_query(query).unsqueeze(1).expand(-1, S, -1)  # [B,S,H]\n",
        "        k = self.w_key(keys)                                    # [B,S,H]\n",
        "        e = torch.tanh(q + k)                                   # [B,S,H]\n",
        "        scores = self.v(e).squeeze(-1)                          # [B,S]\n",
        "        if mask is not None:\n",
        "            neg_inf = torch.finfo(scores.dtype).min\n",
        "            scores = scores.masked_fill(~mask, neg_inf)\n",
        "        attn = torch.softmax(scores, dim=-1)                    # [B,S]\n",
        "        context = torch.bmm(attn.unsqueeze(1), keys).squeeze(1) # [B,H]\n",
        "        return context, attn\n",
        "\n",
        "class AttentionRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, hidden_dim, num_layers, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=PAD)\n",
        "        self.enc = nn.LSTM(\n",
        "            input_size=emb_dim, hidden_size=hidden_dim,\n",
        "            num_layers=num_layers, dropout=dropout,\n",
        "            bidirectional=True, batch_first=True\n",
        "        )\n",
        "        self.dec = nn.LSTM(\n",
        "            input_size=emb_dim + 2*hidden_dim, hidden_size=hidden_dim,\n",
        "            num_layers=num_layers, dropout=dropout, batch_first=True\n",
        "        )\n",
        "        self.dec_hidden_proj = nn.Linear(hidden_dim, 2*hidden_dim)\n",
        "        self.attn = BahdanauAttention(2*hidden_dim)\n",
        "        self.out = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def encode(self, src: Tensor):\n",
        "        # src: [B,S]\n",
        "        x = self.emb(src)                  # [B,S,E]\n",
        "        enc_out, (h, c) = self.enc(x)     # enc_out: [B,S,2H], h: [2L,B,H]\n",
        "        L = self.dec.num_layers\n",
        "        H = self.dec.hidden_size\n",
        "        # reshape & take forward direction\n",
        "        h_ = h.view(L, 2, src.size(0), H)[:,0,:,:].contiguous()  # [L,B,H]\n",
        "        c_ = c.view(L, 2, src.size(0), H)[:,0,:,:].contiguous()  # [L,B,H]\n",
        "        return enc_out, (h_, c_)\n",
        "\n",
        "    def decode_step(self, inp: Tensor, state, enc_out: Tensor, mask: Tensor=None):\n",
        "        # inp: [B,1] token ids, state:(h,c) each [L,B,H], enc_out:[B,S,2H], mask:[B,S]\n",
        "        B = inp.size(0)\n",
        "        emb = self.emb(inp)                # [B,1,E]\n",
        "        h, c = state\n",
        "        q = h[-1]                          # [B,H]\n",
        "        q_proj = self.dec_hidden_proj(q)   # [B,2H]\n",
        "        context, attn = self.attn(q_proj, enc_out, mask)   # [B,2H], [B,S]\n",
        "        dec_in = torch.cat([emb, context.unsqueeze(1)], dim=-1)  # [B,1,E+2H]\n",
        "        out, new_state = self.dec(dec_in, state)           # out: [B,1,H]\n",
        "        logits = self.out(out.squeeze(1))                  # [B,V]\n",
        "        return logits, new_state, attn\n",
        "\n",
        "    def forward(self, src: Tensor, tgt_in: Tensor, mask: Tensor=None):\n",
        "        # Teacher forcing: src [B,S], tgt_in [B,T], returns [B,T,V]\n",
        "        enc_out, state = self.encode(src)\n",
        "        B, T = tgt_in.size()\n",
        "        outs = []\n",
        "        for t in range(T):\n",
        "            inp_t = tgt_in[:,t:t+1]   # [B,1]\n",
        "            logits, state, _ = self.decode_step(inp_t, state, enc_out, mask)\n",
        "            outs.append(logits.unsqueeze(1))\n",
        "        return torch.cat(outs, dim=1)\n",
        "\n",
        "model = AttentionRNN(V, EMBEDDING_DIM, HIDDEN_DIM, NUM_LAYERS, dropout=0.3).to(device)\n",
        "opt = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "print(\"Model ready\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def masked_cross_entropy_logits(logits_flat: Tensor, targets_flat: Tensor, pad_id: int = PAD) -> Tensor:\n",
        "    # logits_flat: [N,V], targets_flat: [N], ignore PAD and OOV\n",
        "    V = logits_flat.size(1)\n",
        "    tgt = targets_flat.long().to(logits_flat.device)\n",
        "    valid = (tgt != pad_id) & (tgt < V)\n",
        "    if valid.sum().item() == 0:\n",
        "        return torch.zeros((), dtype=torch.float32, device=logits_flat.device)\n",
        "    logits_sel  = logits_flat[valid]\n",
        "    targets_sel = tgt[valid]\n",
        "    log_probs = F.log_softmax(logits_sel, dim=-1)  # [M,V]\n",
        "    rows = torch.arange(log_probs.size(0), device=log_probs.device, dtype=torch.long)\n",
        "    nll = -log_probs.view(-1)[rows * V + targets_sel]\n",
        "    return nll.mean()\n",
        "\n",
        "@torch.no_grad()\n",
        "def greedy_decode(model: nn.Module, src_text: str, max_len: int) -> str:\n",
        "    model.eval()\n",
        "    src = torch.tensor([ tokenizer.encode_with_special(src_text, MAX_SEQ_LEN, add_sos=False, add_eos=True) ],\n",
        "                       dtype=torch.long, device=device)  # [1,S]\n",
        "    mask = (src != PAD)                                   # [1,S]\n",
        "    enc_out, state = model.encode(src)\n",
        "    cur = torch.tensor([[SOS]], dtype=torch.long, device=device)\n",
        "    out_ids = []\n",
        "    for _ in range(max_len):\n",
        "        logits, state, _ = model.decode_step(cur, state, enc_out, mask)\n",
        "        nxt = logits.argmax(dim=-1)  # [1]\n",
        "        token_id = int(nxt.item())\n",
        "        if token_id == EOS: break\n",
        "        out_ids.append(token_id)\n",
        "        cur = nxt.view(1,1)\n",
        "    return tokenizer.decode(out_ids)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from collections import Counter\n",
        "import re\n",
        "from typing import List\n",
        "\n",
        "def _tokenize(s: str) -> List[str]:\n",
        "    return [ re.sub(r\"[^0-9a-z]+\", \"\", w) for w in s.lower().split() if re.sub(r\"[^0-9a-z]+\", \"\", w) ]\n",
        "\n",
        "def _ngrams(toks: List[str], n: int) -> Counter:\n",
        "    if n <= 0 or len(toks) < n: return Counter()\n",
        "    grams = [\" \".join(toks[i:i+n]) for i in range(len(toks)-n+1)]\n",
        "    return Counter(grams)\n",
        "\n",
        "def rouge_n(ref: str, cand: str, n: int):\n",
        "    rt = _tokenize(ref); ct = _tokenize(cand)\n",
        "    R = _ngrams(rt, n); C = _ngrams(ct, n)\n",
        "    if sum(R.values()) == 0 or sum(C.values()) == 0:\n",
        "        return dict(p=0.0, r=0.0, f1=0.0)\n",
        "    overlap = sum(min(R[g], C.get(g,0)) for g in R)\n",
        "    p = overlap / max(1, sum(C.values()))\n",
        "    r = overlap / max(1, sum(R.values()))\n",
        "    f1 = 0.0 if (p+r)==0 else 2*p*r/(p+r)\n",
        "    return dict(p=p, r=r, f1=f1)\n",
        "\n",
        "def _lcs_len(a: List[str], b: List[str]) -> int:\n",
        "    n, m = len(a), len(b)\n",
        "    if n==0 or m==0: return 0\n",
        "    prev = [0]*(m+1); curr = [0]*(m+1)\n",
        "    for i in range(1,n+1):\n",
        "        ai = a[i-1]\n",
        "        for j in range(1,m+1):\n",
        "            if ai == b[j-1]:\n",
        "                curr[j] = prev[j-1] + 1\n",
        "            else:\n",
        "                curr[j] = max(prev[j], curr[j-1])\n",
        "        prev, curr = curr, prev\n",
        "    return prev[m]\n",
        "\n",
        "def rouge_l(ref: str, cand: str):\n",
        "    rt = _tokenize(ref); ct = _tokenize(cand)\n",
        "    if not rt or not ct: return dict(p=0.0, r=0.0, f1=0.0)\n",
        "    lcs = _lcs_len(rt, ct)\n",
        "    p = lcs / len(ct)\n",
        "    r = lcs / len(rt)\n",
        "    f1 = 0.0 if (p+r)==0 else 2*p* r / (p+r)\n",
        "    return dict(p=p, r=r, f1=f1)\n",
        "\n",
        "def rouge_all(ref: str, cand: str):\n",
        "    r1 = rouge_n(ref, cand, 1)\n",
        "    r2 = rouge_n(ref, cand, 2)\n",
        "    rl = rouge_l(ref, cand)\n",
        "    return r1, r2, rl\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def train_epoch(model, data, optimizer):\n",
        "    model.train()\n",
        "    total = 0.0; steps = 0\n",
        "    random.shuffle(data)\n",
        "    for batch in tqdm(list(batch_iter(data, BATCH_SIZE))):\n",
        "        src, tgt_in, tgt_out, mask = make_batch(batch, MAX_SEQ_LEN, MAX_SUMMARY_LEN, device)\n",
        "        logits = model(src, tgt_in, mask)         # [B,T,V]\n",
        "        N, T, V = logits.size()\n",
        "        loss = masked_cross_entropy_logits(\n",
        "            logits_flat = logits.reshape(N*T, V),\n",
        "            targets_flat= tgt_out.reshape(N*T),\n",
        "            pad_id=PAD\n",
        "        )\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        total += float(loss.item()); steps += 1\n",
        "    return total / max(1, steps)\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate(model, data):\n",
        "    model.eval()\n",
        "    total = 0.0; steps = 0\n",
        "    for batch in tqdm(list(batch_iter(data, BATCH_SIZE))):\n",
        "        src, tgt_in, tgt_out, mask = make_batch(batch, MAX_SEQ_LEN, MAX_SUMMARY_LEN, device)\n",
        "        logits = model(src, tgt_in, mask)         # [B,T,V]\n",
        "        N, T, V = logits.size()\n",
        "        loss = masked_cross_entropy_logits(\n",
        "            logits_flat = logits.reshape(N*T, V),\n",
        "            targets_flat= tgt_out.reshape(N*T),\n",
        "            pad_id=PAD\n",
        "        )\n",
        "        total += float(loss.item()); steps += 1\n",
        "    return total / max(1, steps)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/251 [00:00<?, ?it/s]"
          ]
        }
      ],
      "source": [
        "\n",
        "best_val = float(\"inf\")\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    print(f\"\\nEpoch {epoch}/{EPOCHS}\")\n",
        "    tr = train_epoch(model, train_data, opt)\n",
        "    va = validate(model, val_data)\n",
        "    ppl = math.exp(va) if va < 20 else float(\"inf\")\n",
        "    print(f\"Train Loss: {tr:.4f} | Val Loss: {va:.4f} | Val PPL: {ppl:.2f}\")\n",
        "    if va < best_val:\n",
        "        best_val = va\n",
        "        torch.save(model.state_dict(), \"best_model.pt\")\n",
        "        print(\"✓ Saved best model\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "@torch.no_grad()\n",
        "def test_samples(n_samples: int = 3):\n",
        "    model.eval()\n",
        "    subset = val_data if len(val_data) >= n_samples else train_data\n",
        "    sample_idx = random.sample(range(len(subset)), k=min(n_samples, len(subset)))\n",
        "    for i, idx in enumerate(sample_idx, 1):\n",
        "        text, ref = subset[idx]\n",
        "        pred = greedy_decode(model, text, MAX_SUMMARY_LEN)\n",
        "        r1, r2, rl = rouge_all(ref, pred)\n",
        "        print(f\"\\n--- Sample {i} ---\")\n",
        "        print(\"Text    :\", (text[:200]+\"...\") if len(text)>200 else text)\n",
        "        print(\"Ref     :\", ref)\n",
        "        print(\"Pred    :\", pred)\n",
        "        print(f\"ROUGE-1 P:{r1['p']:.3f} R:{r1['r']:.3f} F1:{r1['f1']:.3f}\",\n",
        "              f\"| ROUGE-2 P:{r2['p']:.3f} R:{r2['r']:.3f} F1:{r2['f1']:.3f}\",\n",
        "              f\"| ROUGE-L P:{rl['p']:.3f} R:{rl['r']:.3f} F1:{rl['f1']:.3f}\")\n",
        "\n",
        "if os.path.exists(\"best_model.pt\"):\n",
        "    model.load_state_dict(torch.load(\"best_model.pt\", map_location=device))\n",
        "    print(\"✓ Loaded best_model.pt\")\n",
        "else:\n",
        "    print(\"! best_model.pt not found, using current weights\")\n",
        "\n",
        "test_samples(3)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
